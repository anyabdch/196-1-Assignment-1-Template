# LMArena Battle Log

**Student Name:** [Your Name Here]
**Date:** [Date]

## Instructions
For this assignment, you are required to use **LMArena (Battle Mode)** for the exercises below. In Battle Mode, you will see two anonymous models (Model A and Model B) generate answers side-by-side.

For each exercise, record your observations. Your goal is to evaluate **which model provided better code** and explain why you picked it.

---

## Exercise 1: Test-Driven Development (TDD)
*Goal: Which model's code  passed the provided test suite?*

* **Prompt Used:**
    > [Paste the prompt you gave the AI here]

* **Model A Result:**
    * [e.g., Passed all tests / Failed on the birthday function / Syntax Error]
* **Model B Result:**
    * [e.g., Passed all tests / Code was very long / Failed logic]
* **The Winner:** [Model A / Model B / Tie]
* **Which Model Was It?:** [e.g., Gpt o4-mini, Gemini 3 pro, Claude Sonnet 3.5]
* **Reason for Vote:**
    > [Why did you choose this one? Was the code cleaner? Did it handle edge cases better?]

---

## Exercise 2: Test-Last Development (Generating Tests)
*Goal: Which model wrote more thorough tests for the Auth API?*

* **Prompt Used:**
    > [Paste the prompt you gave the AI here]

* **Comparison:**
    * Did Model A cover the "Deactivate Account" edge case? [Yes/No]
    * Did Model B cover the "Deactivate Account" edge case? [Yes/No]
* **The Winner:** [Model A / Model B / Tie]
* **Which Model Was It?:** [e.g., Gpt o4-mini, Gemini 3 pro, Claude Sonnet 3.5]
* **Reason for Vote:**
    > [Which test suite looked more robust? Did one import a library that doesn't exist?]

---

## Exercise 3: Debugging (Basic)
*Goal: Which model explained the "Race Condition" in the threading error better?*

* **Prompt Used:**
    > [Paste the prompt you gave the AI here]

* **Explanation Quality:**
    * **Model A:** [e.g., Did it explain WHY the error happened, or just fix it?]
    * **Model B:** [e.g., Was its explanation clear?]
* **The Winner:** [Model A / Model B / Tie]
* **Which Model Was It?:** [e.g., Gpt o4-mini, Gemini 3 pro, Claude Sonnet 3.5]
* **Reason for Vote:**
    > [I chose this model because...]

---

## Exercise 4: Advanced Debugging
*Goal: Which model found the nuanced logic errors in the currency converter?*

* **Prompt Used:**
    > [Paste the prompt you gave the AI here]

* **Observations:**
    * [Did one model hallucinate a bug that wasn't there? Did one fail to find the math error?]
* **The Winner:** [Model A / Model B / Tie]
* **Which Model Was It?:** [e.g., Gpt o4-mini, Gemini 3 pro, Claude Sonnet 3.5]
* **Reason for Vote:**
    > [I chose this model because...]

---

## Exercise 5: Pseudocode to Code
*Goal: Which model best interpreted your architectural intent?*

* **Pseudocode Snippet Used:**
    > [Paste the specific lines of pseudocode you were testing here]

* **Interpretation:**
    * **Model A Approach:** [e.g., Used a 'While' loop, imported a heavy library]
    * **Model B Approach:** [e.g., Used a recursive function, kept it simple]
* **The Winner:** [Model A / Model B / Tie]
* **Which Model Was It?:** [e.g., Gpt o4-mini, Gemini 3 pro, Claude Sonnet 3.5]
* **Reason for Vote:**
    > [Which implementation matched what you had in your head?]

---

## Final Reflection
* **Overall, did you notice a specific model name (e.g., GPT-4, Claude 3, Llama 3) appearing frequently as the winner after you voted? Was it one you expected?**
    > [Answer here]

* **What was the biggest mistake an AI made during this assignment that you had to manually fix? No big mistakes is also an interesting answer**
    > [Answer here]
